# A1-Terminal - Technische Dokumentation

**Version:** 2.0 (Modulare Architektur)  
**Datum:** November 2025  
**Typ:** Chat-Client fÃ¼r lokale AI-Modelle via Ollama

---

## Inhaltsverzeichnis

1. [Ãœbersicht](#Ã¼bersicht)
2. [Systemanforderungen](#systemanforderungen)
3. [Installation](#installation)
4. [Architektur](#architektur)
5. [Projektstruktur](#projektstruktur)
6. [Kernkomponenten](#kernkomponenten)
7. [Konfiguration](#konfiguration)
8. [API-Referenz](#api-referenz)
9. [BenutzeroberflÃ¤che](#benutzeroberflÃ¤che)
10. [Session-Management](#session-management)
11. [Verwendung](#verwendung)
12. [Entwicklung](#entwicklung)
13. [Troubleshooting](#troubleshooting)

---

## Ãœbersicht

**A1-Terminal** ist ein professioneller, modularer Chat-Client fÃ¼r lokale AI-Modelle, der Ã¼ber die Ollama-API kommuniziert. Die Anwendung bietet eine intuitive grafische BenutzeroberflÃ¤che mit umfangreichen AnpassungsmÃ¶glichkeiten, Session-Management und Echtzeit-Streaming-FunktionalitÃ¤t.

### Hauptmerkmale

- ğŸ¯ **Modulare Architektur** - Saubere Trennung von UI und GeschÃ¤ftslogik
- ğŸš€ **Echtzeit-Streaming** - Live-Anzeige der AI-Antworten wÃ¤hrend der Generierung
- ğŸ’¾ **Session-Management** - Persistente Chat-Sitzungen mit Speicherung und Wiederherstellung
- ğŸ¨ **VollstÃ¤ndig anpassbar** - Farben, Schriftarten, Layout individuell konfigurierbar
- ğŸ“Š **Model-Management** - Download, Auswahl und Kategorisierung von AI-Modellen
- ğŸ”„ **Offline-fÃ¤hig** - Alle Modelle laufen lokal ohne Internetverbindung
- âš¡ **Stop-FunktionalitÃ¤t** - Generierung und Downloads kÃ¶nnen jederzeit gestoppt werden
- ğŸ“ **BIAS-System** - System-Prompts zur Steuerung des AI-Verhaltens

### Technologie-Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CustomTkinter (GUI)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   A1 Terminal Core Application          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ UI-Module    â”‚  Ollama Manager          â”‚
â”‚              â”‚  (API-Client)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Ollama API (localhost:11434)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    Lokale AI-Modelle                    â”‚
â”‚    (llama, mistral, codellama, etc.)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Systemanforderungen

### Software

- **Python:** 3.8 oder hÃ¶her
- **Ollama:** Neueste Version (lÃ¤uft als Hintergrunddienst)
- **Betriebssystem:** Windows, Linux oder macOS

### Hardware (Mindestanforderungen)

- **RAM:** 8 GB (16 GB empfohlen fÃ¼r grÃ¶ÃŸere Modelle)
- **Speicher:** 10 GB freier Festplattenspeicher
- **CPU:** Multi-Core Prozessor empfohlen

### Modell-spezifische Anforderungen

| ModellgrÃ¶ÃŸe | RAM-Bedarf | Beispiele |
|-------------|------------|-----------|
| ğŸŸ¢ Klein (< 4GB) | 4-8 GB | tinyllama:1.1b, phi3:mini, gemma:2b |
| ğŸŸ¡ Mittel (4-8GB) | 8-12 GB | llama3.2:3b, mistral:7b, codellama:7b |
| ğŸŸ  GroÃŸ (8-16GB) | 16-24 GB | llama2:13b, codellama:13b |
| ğŸ”´ Sehr GroÃŸ (16GB+) | 32+ GB | llama2:70b, mixtral:8x7b |

---

## Installation

### 1. Ollama installieren

Besuchen Sie [ollama.ai](https://ollama.ai) und installieren Sie Ollama fÃ¼r Ihr Betriebssystem.

**Verifizierung:**
```powershell
ollama --version
```

### 2. Repository klonen

```powershell
cd "C:\Users\<Benutzername>\Documents"
git clone https://github.com/Nr44suessauer/Ki-whisperer.git
cd Ki-whisperer\a1_terminal_modular
```

### 3. AbhÃ¤ngigkeiten installieren

```powershell
pip install -r requirements.txt
```

**requirements.txt:**
```
customtkinter>=5.2.0
ollama>=0.1.0
PyYAML>=6.0
requests>=2.31.0
pyperclip>=1.8.2
```

### 4. Ollama starten

```powershell
ollama serve
```

Ollama lÃ¤uft auf `http://localhost:11434`

---

## Architektur

### Design-Prinzipien

1. **ModularitÃ¤t** - Jede Komponente hat eine klare Verantwortlichkeit
2. **Separation of Concerns** - UI und GeschÃ¤ftslogik sind getrennt
3. **Konfigurierbarkeit** - Alle Einstellungen Ã¼ber YAML-Konfiguration
4. **Erweiterbarkeit** - Neue UI-Komponenten kÃ¶nnen einfach hinzugefÃ¼gt werden

### Schichtenarchitektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  main.py (Entry Point)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  core/a1_terminal.py             â”‚
â”‚  (Hauptanwendungsklasse)         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ui/          â”‚   â”‚  core/          â”‚
â”‚  - chat_bubbleâ”‚   â”‚  - ollama_mgr   â”‚
â”‚  - session_cardâ”‚  â”‚                 â”‚
â”‚  - model_sel  â”‚   â”‚                 â”‚
â”‚  - etc.       â”‚   â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Projektstruktur

```
Ki-whisperer/
â”œâ”€â”€ a1_terminal_modular/                    # âœ¨ Hauptanwendung
â”‚   â”œâ”€â”€ main.py                             # Entry Point
â”‚   â”œâ”€â”€ start.bat                           # Windows Start-Skript
â”‚   â”œâ”€â”€ restart.py                          # Neustart-Helfer
â”‚   â”œâ”€â”€ requirements.txt                    # Python-AbhÃ¤ngigkeiten
â”‚   â”œâ”€â”€ a1_terminal_config.yaml             # Konfigurationsdatei
â”‚   â”œâ”€â”€ sessions.json                       # Session-Metadaten
â”‚   â”‚
â”‚   â”œâ”€â”€ sessions/                           # Gespeicherte Chat-Sessions
â”‚   â”‚   â””â”€â”€ Session_<datum>_<zeit>.json
â”‚   â”‚
â”‚   â””â”€â”€ src/                                # Quellcode
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”‚
â”‚       â”œâ”€â”€ core/                           # Kernlogik
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ a1_terminal.py              # Hauptklasse (4200+ Zeilen)
â”‚       â”‚   â””â”€â”€ ollama_manager.py           # Ollama-API-Client
â”‚       â”‚
â”‚       â””â”€â”€ ui/                             # UI-Komponenten
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ chat_bubble.py              # Chat-Nachrichtenanzeige
â”‚           â”œâ”€â”€ enhanced_chat_bubble.py     # Erweiterte Chat-Bubbles
â”‚           â”œâ”€â”€ session_card.py             # Session-Karten
â”‚           â”œâ”€â”€ model_selector.py           # Model-Auswahl-Widget
â”‚           â”œâ”€â”€ model_info_dropdown.py      # Model-Info-Anzeige
â”‚           â”œâ”€â”€ categorized_combobox.py     # Kategorisierte Dropdown
â”‚           â”œâ”€â”€ color_wheel.py              # Farbauswahl-Widget
â”‚           â”œâ”€â”€ resizable_pane.py           # GrÃ¶ÃŸenÃ¤nderbare Panels
â”‚           â”œâ”€â”€ modern_ui.py                # Modernes UI-Design
â”‚           â””â”€â”€ ultimate_ui.py              # Ultimate UI-Setup
â”‚
â”œâ”€â”€ ki_whisperer_config.yaml                # Legacy-Konfiguration
â””â”€â”€ README.md                               # Dokumentation (zu lÃ¶schen)
```

---

## Kernkomponenten

### 1. A1Terminal (core/a1_terminal.py)

**Hauptklasse** der Anwendung mit ~4200 Zeilen Code.

#### Hauptverantwortlichkeiten:
- Initialisierung der Anwendung
- UI-Setup und Event-Handling
- Session-Management
- Chat-Historie-Verwaltung
- Konfigurationsverwaltung

#### Wichtige Methoden:

```python
class A1Terminal:
    def __init__(self):
        """Initialisiert die Anwendung"""
        
    def setup_ui(self):
        """Erstellt die BenutzeroberflÃ¤che"""
        
    def send_message(self):
        """Sendet eine Nachricht an das AI-Modell"""
        
    def generate_response(self, model, messages):
        """Generiert eine AI-Antwort (mit Streaming)"""
        
    def save_session(self):
        """Speichert die aktuelle Session"""
        
    def load_session(self, session_id):
        """LÃ¤dt eine gespeicherte Session"""
        
    def load_config(self):
        """LÃ¤dt die YAML-Konfiguration"""
        
    def save_config(self, config):
        """Speichert die YAML-Konfiguration"""
```

#### Wichtige Attribute:

```python
self.root              # CTk Hauptfenster
self.ollama            # OllamaManager Instanz
self.current_model     # Aktuell ausgewÃ¤hltes Modell
self.chat_history      # Liste der Chat-Nachrichten
self.chat_bubbles      # Liste der UI Chat-Bubbles
self.sessions          # Dict aller Sessions
self.current_session_id # ID der aktuellen Session
self.config            # Konfiguration (Dict)
self.generation_stopped # Flag fÃ¼r Stop-FunktionalitÃ¤t
```

---

### 2. OllamaManager (core/ollama_manager.py)

**API-Client** fÃ¼r die Kommunikation mit dem Ollama-Server.

#### Hauptverantwortlichkeiten:
- Verbindung zu Ollama-API
- Modell-Download und -Verwaltung
- Streaming-Antworten
- Modell-Kategorisierung

#### Wichtige Methoden:

```python
class OllamaManager:
    def __init__(self):
        """Initialisiert den Ollama-Client"""
        self.base_url = "http://localhost:11434"
        self.client = ollama.Client()
    
    def is_ollama_running(self):
        """PrÃ¼ft ob Ollama-Server lÃ¤uft"""
        
    def get_available_models(self):
        """Holt lokal installierte Modelle"""
        
    def get_all_ollama_models(self):
        """Holt alle verfÃ¼gbaren Modelle von Registry"""
        
    def download_model(self, model_name, progress_callback):
        """LÃ¤dt ein Modell mit Progress-Tracking herunter"""
        
    def categorize_models_by_size(self, models):
        """Kategorisiert Modelle nach RAM-Bedarf"""
        
    def generate_response(self, model, messages, stream=True):
        """Generiert AI-Antwort mit Streaming"""
```

#### Modell-Kategorisierung:

```python
categories = {
    "ğŸŸ¢ Klein (< 4GB RAM)": [],      # 0.5b-3b Parameter
    "ğŸŸ¡ Mittel (4-8GB RAM)": [],     # 7b Parameter
    "ğŸŸ  GroÃŸ (8-16GB RAM)": [],      # 13b-15b Parameter
    "ğŸ”´ Sehr GroÃŸ (16GB+ RAM)": []   # 70b+ Parameter
}
```

---

### 3. UI-Komponenten (ui/)

#### ChatBubble (chat_bubble.py)
Darstellung einzelner Chat-Nachrichten mit Rolle-basiertem Styling.

```python
class ChatBubble:
    def __init__(self, parent, message, role, config):
        """
        Args:
            parent: Parent-Widget
            message: Nachrichtentext
            role: "user", "assistant", "system"
            config: Konfigurations-Dict
        """
```

#### SessionCard (session_card.py)
Visuelle Darstellung einer Chat-Session in der Sidebar.

```python
class SessionCard:
    def __init__(self, parent, session_id, session_data, on_select, on_delete):
        """
        Args:
            session_id: Eindeutige Session-ID
            session_data: Session-Metadaten (Dict)
            on_select: Callback beim Anklicken
            on_delete: Callback beim LÃ¶schen
        """
```

#### ModelSelector (model_selector.py)
Widget zur Modell-Auswahl mit Kategorisierung.

```python
class ModelSelector:
    def __init__(self, parent, models, on_select):
        """
        Args:
            models: Liste verfÃ¼gbarer Modelle
            on_select: Callback bei Modell-Auswahl
        """
```

#### ColorWheel (color_wheel.py)
Farbauswahl-Widget fÃ¼r UI-Customization.

#### ResizablePane (resizable_pane.py)
GrÃ¶ÃŸenÃ¤nderbare Panel-Komponente fÃ¼r flexible Layouts.

---

## Konfiguration

### Konfigurationsdatei: a1_terminal_config.yaml

Die Konfiguration wird automatisch erstellt und beim Starten geladen. Alle Ã„nderungen in der GUI werden persistent gespeichert.

#### Struktur:

```yaml
# ========== BUBBLE-FARBEN ==========
bubble_colors:
  user_bg_color: "#003300"        # Matrix-GrÃ¼n
  user_text_color: "#00FF00"
  ai_bg_color: "#1E3A5F"          # Dunkelblau
  ai_text_color: "white"
  system_bg_color: "#722F37"      # Dunkelrot
  system_text_color: "white"

# ========== SCHRIFTARTEN ==========
fonts:
  user_font: "Courier New"
  user_font_size: 11
  ai_font: "Consolas"
  ai_font_size: 11
  system_font: "Arial"
  system_font_size: 10

# ========== UI-LAYOUT ==========
ui_session_panel_width: 350
ui_window_width: 1400
ui_window_height: 900
ui_padding_main: 10
ui_padding_content: 5

# ========== CHAT-DISPLAY ==========
ui_chat_bubble_corner_radius: 10
ui_chat_bubble_padding_x: 15
ui_chat_bubble_padding_y: 10
ui_chat_spacing: 10
ui_chat_max_width_ratio: 0.8

# ========== FARBEN & THEME ==========
ui_bg_color: "#1a1a1a"
ui_fg_color: "#2b2b2b"
ui_accent_color: "#2B8A3E"
ui_hover_color: "#37A24B"
ui_text_color: "white"
ui_border_color: "#3a3a3a"

# ========== OPTIONEN ==========
show_system_messages: true
auto_scroll_chat: true
show_timestamps: true
compact_mode: false
```

### Konfiguration programmatisch Ã¤ndern:

```python
# Konfiguration laden
config = app.load_config()

# Werte Ã¤ndern
config['user_bg_color'] = '#FF0000'
config['ui_window_width'] = 1600

# Speichern
app.save_config(config)
```

---

## API-Referenz

### Ollama API Endpoints

A1-Terminal kommuniziert mit folgenden Ollama-Endpoints:

#### 1. Status prÃ¼fen
```
GET http://localhost:11434/api/tags
```

#### 2. Modelle auflisten
```
GET http://localhost:11434/api/tags
Response: {
  "models": [
    {
      "name": "llama3.2:3b",
      "modified_at": "2024-11-10T12:00:00Z",
      "size": 2000000000
    }
  ]
}
```

#### 3. Modell herunterladen
```
POST http://localhost:11434/api/pull
Body: {
  "name": "llama3.2:3b",
  "stream": true
}
```

#### 4. Chat-Anfrage (Streaming)
```
POST http://localhost:11434/api/chat
Body: {
  "model": "llama3.2:3b",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "stream": true
}
```

### Python Ollama Client

A1-Terminal nutzt den offiziellen `ollama` Python-Client:

```python
import ollama

# Client initialisieren
client = ollama.Client()

# Chat mit Streaming
response = client.chat(
    model='llama3.2:3b',
    messages=[
        {'role': 'user', 'content': 'Hello!'}
    ],
    stream=True
)

for chunk in response:
    if 'message' in chunk:
        print(chunk['message']['content'], end='')
```

---

## BenutzeroberflÃ¤che

### Layout-Struktur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hauptfenster (1400x900 px)                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  Session     â”‚  â”‚  Model Selector                  â”‚   â”‚
â”‚  Panel       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  (350px)     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚              â”‚  â”‚                                  â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  Chat Display Area               â”‚   â”‚
â”‚  â”‚Session â”‚  â”‚  â”‚  (Auto-Scroll)                   â”‚   â”‚
â”‚  â”‚Card 1  â”‚  â”‚  â”‚                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚                                  â”‚   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”‚Session â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚Card 2  â”‚  â”‚  â”‚  Message Input (40px)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â”‚  [Send] [Stop] [Clear]                  â”‚
â”‚  [New]       â”‚                                          â”‚
â”‚  [Delete]    â”‚  Tabs: Chat | Config | Models | Debug   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tab-System

#### 1. Chat-Tab
- Hauptbereich fÃ¼r Chat-Konversation
- Message Input mit Multi-Line-Support
- Buttons: Send, Stop, Clear
- Model Selector Dropdown

#### 2. Config-Tab
- Farbauswahl fÃ¼r User/AI/System
- Schriftart-Einstellungen
- Layout-Anpassungen
- Speichern/Laden von Presets

#### 3. Models-Tab
- Liste aller verfÃ¼gbaren Modelle
- Download-Funktion mit Progress-Bar
- Modell-Info (GrÃ¶ÃŸe, Parameter)
- Kategorisierung nach RAM-Bedarf

#### 4. Debug-Tab
- Logs und System-Meldungen
- Performance-Metriken
- API-Status
- Session-Daten-Inspektion

### Keyboard Shortcuts

| Shortcut | Funktion |
|----------|----------|
| `Enter` | Nachricht senden |
| `Shift+Enter` | Neue Zeile im Input |
| `â†‘` / `â†“` | Nachrichtenverlauf durchsuchen |
| `Ctrl+L` | Chat leeren |
| `Ctrl+N` | Neue Session |
| `Ctrl+S` | Session speichern |
| `Escape` | Generation stoppen |

---

## Session-Management

### Session-Struktur

Jede Session wird als JSON-Datei gespeichert:

```json
{
  "session_id": "session_20251110_153045_123",
  "title": "Chat Ã¼ber Python",
  "model": "llama3.2:3b",
  "created_at": "2025-11-10T15:30:45",
  "last_modified": "2025-11-10T15:45:30",
  "bias": "Du bist ein hilfreicher Python-Experte.",
  "messages": [
    {
      "role": "user",
      "content": "Wie erstelle ich eine Liste?",
      "timestamp": "2025-11-10T15:31:00"
    },
    {
      "role": "assistant",
      "content": "Du kannst eine Liste mit [] erstellen...",
      "timestamp": "2025-11-10T15:31:05"
    }
  ]
}
```

### Session-Metadaten (sessions.json)

```json
{
  "sessions": {
    "session_20251110_153045_123": {
      "title": "Chat Ã¼ber Python",
      "model": "llama3.2:3b",
      "created_at": "2025-11-10T15:30:45",
      "last_modified": "2025-11-10T15:45:30",
      "message_count": 4,
      "file_path": "sessions/Session_10.11_15-30_session_20251110_153045_123.json"
    }
  },
  "current_session": "session_20251110_153045_123"
}
```

### Session-Operationen

#### Neue Session erstellen:
```python
app.create_new_session()
```

#### Session laden:
```python
app.load_session(session_id)
```

#### Session speichern:
```python
app.save_session()
```

#### Session lÃ¶schen:
```python
app.delete_session(session_id)
```

### Auto-Save

Sessions werden automatisch gespeichert:
- Nach jeder Nachricht (mit 2s VerzÃ¶gerung)
- Beim SchlieÃŸen der Anwendung
- Beim Wechsel der Session

---

## Verwendung

### Schnellstart

#### 1. Anwendung starten

**Windows:**
```powershell
cd a1_terminal_modular
.\start.bat
```

**Oder manuell:**
```powershell
python main.py
```

#### 2. Modell auswÃ¤hlen

- Klicken Sie auf das Model-Selector Dropdown
- WÃ¤hlen Sie ein installiertes Modell
- Oder laden Sie ein neues Modell im "Models"-Tab herunter

#### 3. Chat beginnen

- Geben Sie Ihre Nachricht im Input-Feld ein
- DrÃ¼cken Sie `Enter` oder klicken Sie auf "Send"
- Die AI-Antwort erscheint in Echtzeit (Streaming)

### BIAS-System

BIAS ist ein System-Prompt, der das Verhalten der AI steuert:

**Beispiele:**

```
Du bist ein hilfsbereiter Python-Programmierer.
```

```
Antworte immer auf Deutsch und sei prÃ¤zise.
```

```
Du bist ein Experte fÃ¼r Machine Learning.
ErklÃ¤re Konzepte einfach und mit Beispielen.
```

**BIAS setzen:**
1. Geben Sie den BIAS-Text im Textfeld unten links ein
2. Der BIAS wird automatisch bei jeder Anfrage mitgesendet
3. Ã„nderungen werden per Auto-Save gespeichert

### Modell-Download

1. Wechseln Sie zum "Models"-Tab
2. Klicken Sie auf "Refresh Model Library"
3. WÃ¤hlen Sie ein Modell aus der Liste
4. Klicken Sie auf "Download Selected Model"
5. Progress-Bar zeigt den Fortschritt
6. Nach Abschluss ist das Modell verfÃ¼gbar

**Download stoppen:**
- Klicken Sie auf "Stop Download"

### Chat-Befehle

| Befehl | Beschreibung |
|--------|--------------|
| `Send` | Nachricht senden |
| `Stop` | Generierung stoppen |
| `Clear` | Chat leeren (behÃ¤lt Session) |
| `New Session` | Neue Session erstellen |
| `Delete Session` | Aktuelle Session lÃ¶schen |

---

## Entwicklung

### Entwicklungsumgebung einrichten

```powershell
# Repository klonen
git clone https://github.com/Nr44suessauer/Ki-whisperer.git
cd Ki-whisperer/a1_terminal_modular

# Virtual Environment erstellen
python -m venv venv
.\venv\Scripts\Activate.ps1

# Dependencies installieren
pip install -r requirements.txt

# Anwendung starten
python main.py
```

### Neue UI-Komponente hinzufÃ¼gen

1. **Erstellen Sie eine neue Datei** in `src/ui/`:
```python
# src/ui/my_component.py
import customtkinter as ctk

class MyComponent(ctk.CTkFrame):
    def __init__(self, parent, **kwargs):
        super().__init__(parent, **kwargs)
        self.setup_ui()
    
    def setup_ui(self):
        # UI erstellen
        pass
```

2. **Importieren** in `a1_terminal.py`:
```python
from src.ui.my_component import MyComponent
```

3. **Verwenden** in `setup_ui()`:
```python
self.my_component = MyComponent(self.root)
self.my_component.pack()
```

### Konfigurationsparameter hinzufÃ¼gen

1. **Standard-Wert** in `get_default_config()` definieren:
```python
def get_default_config(self):
    return {
        # ... existing config ...
        "my_new_parameter": "default_value",
    }
```

2. **Verwenden** in der Anwendung:
```python
value = self.config.get('my_new_parameter', 'fallback_value')
```

3. **Speichern** bei Ã„nderung:
```python
self.config['my_new_parameter'] = new_value
self.save_config(self.config)
```

### Code-Stil

- **PEP 8** konform
- **Docstrings** fÃ¼r alle Klassen und Methoden
- **Type Hints** wo mÃ¶glich
- **Fehlerbehandlung** mit try-except
- **Logging** mit print() fÃ¼r Debug-Ausgaben

### Testing

```python
# Ollama-Verbindung testen
if not self.ollama.is_ollama_running():
    print("âŒ Ollama ist nicht erreichbar")

# Modell-Download testen
self.ollama.download_model(
    "tinyllama:1.1b",
    progress_callback=lambda p: print(f"Progress: {p}%")
)

# Config testen
config = self.load_config()
assert 'user_bg_color' in config
```

---

## Troubleshooting

### Problem: Ollama nicht erreichbar

**Symptom:** "Ollama ist nicht erreichbar" beim Start

**LÃ¶sung:**
```powershell
# Ollama starten
ollama serve

# In neuem Terminal testen
ollama list
```

### Problem: Modell-Download schlÃ¤gt fehl

**Symptom:** Download bricht ab oder friert ein

**LÃ¶sung:**
1. Internetverbindung prÃ¼fen
2. Festplattenspeicher prÃ¼fen
3. Ollama neu starten
4. Download erneut versuchen

### Problem: UI wird nicht korrekt angezeigt

**Symptom:** Komponenten fehlen oder sind falsch positioniert

**LÃ¶sung:**
1. Konfigurationsdatei lÃ¶schen (wird neu erstellt)
```powershell
Remove-Item a1_terminal_config.yaml
```
2. Anwendung neu starten

### Problem: Session kann nicht geladen werden

**Symptom:** Fehler beim Laden einer gespeicherten Session

**LÃ¶sung:**
1. PrÃ¼fen Sie die Session-Datei auf Syntaxfehler
2. Backup wiederherstellen falls vorhanden
3. Session neu erstellen

### Problem: Hoher RAM-Verbrauch

**Symptom:** System wird langsam bei groÃŸen Modellen

**LÃ¶sung:**
1. Kleineres Modell wÃ¤hlen (z.B. tinyllama statt llama2:70b)
2. Andere Anwendungen schlieÃŸen
3. Swap-Space erhÃ¶hen (Linux/macOS)

### Logs und Debug-Informationen

**Debug-Modus aktivieren:**
Wechseln Sie zum "Debug"-Tab fÃ¼r detaillierte Logs.

**Console-Ausgaben:**
Alle Systemausgaben erscheinen im Terminal-Fenster, von dem die Anwendung gestartet wurde.

---

## Anhang

### UnterstÃ¼tzte Modelle (Auswahl)

#### Kleine Modelle (< 4GB RAM)
- `tinyllama:1.1b` - Sehr schnell, einfache Aufgaben
- `phi3:mini` - Microsoft, gute QualitÃ¤t
- `gemma:2b` - Google, balanced
- `qwen2:1.5b` - Alibaba, mehrsprachig

#### Mittlere Modelle (4-8GB RAM)
- `llama3.2:3b` - Meta, neueste Version
- `mistral:7b` - Mistral AI, sehr gut
- `codellama:7b` - Meta, spezialisiert auf Code
- `gemma:7b` - Google, balanced

#### GroÃŸe Modelle (8-16GB RAM)
- `llama2:13b` - Meta, hohe QualitÃ¤t
- `codellama:13b` - Erweiterte Code-FÃ¤higkeiten
- `vicuna:13b` - Community fine-tune

#### Sehr groÃŸe Modelle (16GB+ RAM)
- `llama2:70b` - Top-QualitÃ¤t
- `mixtral:8x7b` - Mixture-of-Experts
- `codellama:34b` - Professional Code

### Performance-Tipps

1. **ModellgrÃ¶ÃŸe wÃ¤hlen:**
   - FÃ¼r Chat: 7b Modelle ausreichend
   - FÃ¼r Code: codellama empfohlen
   - FÃ¼r QualitÃ¤t: 13b+ Modelle

2. **RAM optimieren:**
   - SchlieÃŸen Sie unnÃ¶tige Programme
   - Verwenden Sie kleinere Modelle fÃ¼r schnelle Tests
   - Aktivieren Sie Swap/Pagefile

3. **Ollama optimieren:**
   - Starten Sie Ollama neu bei Problemen
   - LÃ¶schen Sie ungenutzte Modelle: `ollama rm <model>`
   - Cache leeren falls nÃ¶tig

### Ressourcen

- **Ollama Dokumentation:** https://ollama.ai/docs
- **GitHub Repository:** https://github.com/Nr44suessauer/Ki-whisperer
- **CustomTkinter Docs:** https://github.com/TomSchimansky/CustomTkinter
- **Python Ollama Client:** https://github.com/ollama/ollama-python

### Lizenz

Dieses Projekt ist Open Source. Siehe Repository fÃ¼r Details.

### Kontakt

Bei Fragen oder Problemen erstellen Sie bitte ein Issue im GitHub Repository.

---

**Letzte Aktualisierung:** November 2025  
**Dokumentations-Version:** 1.0

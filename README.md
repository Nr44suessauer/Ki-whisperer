# A1-Terminal# A1-Terminal



**Professioneller Chat-Client fÃ¼r lokale AI-Modelle via Ollama****Professioneller Chat-Client fÃ¼r lokale AI-Modelle via Ollama**



Version 2.0 - Modulare ArchitekturVersion 2.0 - Modulare Architektur



---## ğŸ“ Projektstruktur



## ğŸ“– Dokumentation---



Die vollstÃ¤ndige technische Dokumentation finden Sie hier:Das Projekt ist jetzt **modular** aufgebaut:



**â¡ï¸ [DOKUMENTATION.md](./DOKUMENTATION.md)**## ğŸ“‹ Inhaltsverzeichnis



---```



## ğŸš€ Schnellstart1. [Ãœberblick](#Ã¼berblick)Ki-whisperer/



### 1. Ollama installieren2. [System-Architektur](#system-architektur)â”œâ”€â”€ a1_terminal_modular/     # âœ¨ NEUE MODULARE VERSION (empfohlen)

Besuchen Sie [ollama.ai](https://ollama.ai) und installieren Sie Ollama.

3. [Installation](#installation)â”‚   â”œâ”€â”€ start.bat            # Einfach doppelklicken zum Starten!

### 2. Dependencies installieren

```powershell4. [Schnellstart](#schnellstart)â”‚   â”œâ”€â”€ main.py

cd a1_terminal_modular

pip install -r requirements.txt5. [Modulare Struktur](#modulare-struktur)â”‚   â””â”€â”€ src/

```

6. [Technische Dokumentation](#technische-dokumentation)â”‚       â”œâ”€â”€ ui/              # UI-Komponenten

### 3. Starten

```powershell7. [Features](#features)â”‚       â””â”€â”€ core/            # Kernlogik

.\start.bat

```8. [Konfiguration](#konfiguration)â”‚



oder9. [API-Referenz](#api-referenz)â””â”€â”€ OLD_VERSION/             # Archivierte alte Version



```powershell10. [Performance & Monitoring](#performance--monitoring)    â””â”€â”€ llm_messenger.py     # Original monolithische Datei

python main.py

```11. [Troubleshooting](#troubleshooting)```



---



## âœ¨ Features---## Installation



- ğŸ¯ Modulare Architektur

- ğŸš€ Echtzeit-Streaming

- ğŸ’¾ Session-Management## Ãœberblick1. **Repository klonen oder herunterladen**

- ğŸ¨ VollstÃ¤ndig anpassbar

- ğŸ“Š Model-Management   ```bash

- ğŸ”„ Offline-fÃ¤hig

**A1-Terminal** ist ein moderner, modularer Chat-Client fÃ¼r lokale AI-Modelle, der Ã¼ber die Ollama-API kommuniziert. Die Anwendung bietet eine intuitive GUI mit umfangreichen AnpassungsmÃ¶glichkeiten, Session-Management und Echtzeit-Streaming-FunktionalitÃ¤t.   cd "C:\Users\marcn\Documents\Ki-whisperer"

---

   ```

Weitere Informationen, API-Referenz, Troubleshooting und mehr in der **[vollstÃ¤ndigen Dokumentation](./DOKUMENTATION.md)**.

### Hauptmerkmale

2. **AbhÃ¤ngigkeiten installieren**

- ğŸ¯ **Modular & Wartbar** - Saubere Architektur mit klarer Trennung   ```bash

- ğŸš€ **Echtzeit-Streaming** - Live-Anzeige der AI-Antworten   cd a1_terminal_modular

- ğŸ’¾ **Session-Management** - Persistente Chat-Sitzungen   pip install -r requirements.txt

- ğŸ¨ **VollstÃ¤ndig anpassbar** - Farben, Fonts, Layout   ```

- ğŸ“Š **Monitoring** - Performance- und Nutzungsstatistiken

- ğŸ”„ **Model-Management** - Download, Auswahl, Kategorisierung## ğŸš€ Verwendung (Neue Version)



### Technologie-Stack1. **Ollama starten** (falls noch nicht gestartet)

   ```bash

```   ollama serve

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   ```

â”‚         CustomTkinter (GUI)             â”‚

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤2. **A1 Terminal starten**

â”‚   A1 Terminal Core Application          â”‚   ```bash

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   cd a1_terminal_modular

â”‚ UI-Module    â”‚  Ollama Manager          â”‚   start.bat

â”‚              â”‚  (API-Client)            â”‚   ```

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   

â”‚         Ollama API (localhost:11434)    â”‚   Oder direkt:

â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   ```bash

â”‚    Lokale AI-Modelle (llama, mistral,  â”‚   python main.py

â”‚      codellama, gemma, phi, etc.)       â”‚   ```

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```## Erste Schritte



---1. **Modell herunterladen**

   - Klicken Sie auf "Modell herunterladen"

## System-Architektur   - Geben Sie einen Modellnamen ein (z.B. `llama2`, `mistral`, `codellama`)

   - Warten Sie, bis der Download abgeschlossen ist

### GesamtÃ¼bersicht

2. **Modell auswÃ¤hlen**

```   - WÃ¤hlen Sie das gewÃ¼nschte Modell aus dem Dropdown-MenÃ¼

A1-Terminal/

â”‚3. **Chatten**

â”œâ”€â”€ a1_terminal_modular/          # Hauptanwendung   - Geben Sie Ihre Nachricht ein und drÃ¼cken Sie Enter oder klicken Sie "Senden"

â”‚   â”œâ”€â”€ main.py                   # Einstiegspunkt   - Die AI antwortet in Echtzeit

â”‚   â”œâ”€â”€ start.bat                 # Windows-Launcher

â”‚   â”œâ”€â”€ requirements.txt          # Python-Dependencies## Beliebte Modelle

â”‚   â”‚

â”‚   â”œâ”€â”€ src/- **llama2** - Allzweck-Sprachmodell von Meta

â”‚   â”‚   â”œâ”€â”€ core/                 # Kernlogik- **mistral** - Schnelles und effizientes Modell

â”‚   â”‚   â”‚   â”œâ”€â”€ a1_terminal.py    # Hauptanwendung (3200+ Zeilen)- **codellama** - Spezialisiert auf Programmierung

â”‚   â”‚   â”‚   â””â”€â”€ ollama_manager.py # API-Client (320 Zeilen)- **phi** - Kleines, aber leistungsstarkes Modell

â”‚   â”‚   â”‚- **gemma** - Google's offenes Modell

â”‚   â”‚   â””â”€â”€ ui/                   # UI-Komponenten

â”‚   â”‚       â”œâ”€â”€ color_wheel.py    # FarbwÃ¤hler (194 Zeilen)## Funktionen

â”‚   â”‚       â”œâ”€â”€ chat_bubble.py    # Chat-Nachricht (264 Zeilen)

â”‚   â”‚       â””â”€â”€ categorized_combobox.py  # Dropdown (60 Zeilen)### Modell-Management

â”‚   â”‚- **Download**: Laden Sie neue Modelle direkt herunter

â”‚   â””â”€â”€ sessions/                 # Session-Daten (JSON)- **LÃ¶schen**: Entfernen Sie nicht benÃ¶tigte Modelle

â”‚- **AuswÃ¤hlen**: Wechseln Sie zwischen verschiedenen Modellen

â”œâ”€â”€ a1_terminal_config.yaml      # Zentrale Konfiguration

â””â”€â”€ OLD_VERSION/                  # Archiv (monolithische Version)### Chat-Features

```- **Streaming**: Sehen Sie die Antwort in Echtzeit

- **Historie**: Chat-Verlauf bleibt wÃ¤hrend der Session erhalten

### Komponenten-Diagramm- **Zeitstempel**: Alle Nachrichten haben Zeitstempel

- **System-Meldungen**: Informationen Ã¼ber Status und Fehler

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”### ğŸ¨ Konfiguration & Anpassung

â”‚                    main.py                          â”‚- **Config-Tab**: VollstÃ¤ndig anpassbare BenutzeroberflÃ¤che

â”‚              (Application Entry Point)              â”‚- **Fixierte Buttons**: "Anwenden" und "Standard" buttons immer am unteren Rand sichtbar

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- **RGB-FarbwÃ¤hler**: Klick auf ğŸ¨-Buttons Ã¶ffnet visuellen FarbwÃ¤hler

                   â”‚- **Farb-Preview-Icons**: Live-Vorschau der gewÃ¤hlten Farben mit farbigen Quadraten

                   â–¼- **Komprimiertes Layout**: Mehr Optionen nebeneinander fÃ¼r bessere Ãœbersicht

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- **Live-Updates**: Farb-Previews aktualisieren sich beim Tippen

â”‚               A1Terminal                            â”‚- **Individuelle Farben**: Separate Farben fÃ¼r User, AI und System-Nachrichten

â”‚          (Core Application Class)                   â”‚- **Schriftarten**: Anpassbare Fonts und SchriftgrÃ¶ÃŸen

â”‚                                                     â”‚- **Reset-Funktion**: ZurÃ¼cksetzen auf Standardwerte

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

â”‚  â”‚         UI-Management                        â”‚ â”‚## ğŸ“– Verwendung

â”‚  â”‚  â€¢ Tabs (Chat, Config, BIAS, Export)        â”‚ â”‚

â”‚  â”‚  â€¢ Event-Handling                           â”‚ â”‚### Konfiguration anpassen

â”‚  â”‚  â€¢ Layout-Orchestrierung                    â”‚ â”‚1. **Config-Tab Ã¶ffnen**: Klicken Sie auf den "Config" Reiter

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚2. **Farben wÃ¤hlen**:

â”‚                                                     â”‚   - **Farb-Preview**: Farbige Quadrate â–  zeigen aktuelle Farben

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   - **Manuelle Eingabe**: Geben Sie Hex-Codes direkt ein (#FF0000) ODER

â”‚  â”‚      Session-Management                      â”‚ â”‚   - **RGB-FarbwÃ¤hler**: Klicken Sie auf ğŸ¨-Buttons fÃ¼r visuellen FarbwÃ¤hler

â”‚  â”‚  â€¢ Laden/Speichern von Sessions             â”‚ â”‚   - **Live-Updates**: Preview-Icons aktualisieren sich beim Tippen

â”‚  â”‚  â€¢ BIAS-System                              â”‚ â”‚3. **Schriftarten**: WÃ¤hlen Sie aus horizontalen Dropdown-MenÃ¼s

â”‚  â”‚  â€¢ Auto-Save (60s Intervall)                â”‚ â”‚4. **Kompakte Ansicht**: Alle Optionen Ã¼bersichtlich nebeneinander

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚5. **Fixierte Buttons**: "âœ… Anwenden" und "ğŸ”„ Standard" bleiben beim Scrollen immer am unteren Rand sichtbar

â”‚                                                     â”‚6. **Schnelle Anwendung**: Buttons immer erreichbar ohne Scrollen

â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚

â”‚  â”‚    Konfigurations-Management                 â”‚ â”‚## Fehlerbehebung

â”‚  â”‚  â€¢ YAML-Laden/Speichern                     â”‚ â”‚

â”‚  â”‚  â€¢ Live-Updates                             â”‚ â”‚### Ollama nicht verbunden

â”‚  â”‚  â€¢ Standard-Werte                           â”‚ â”‚- Stellen Sie sicher, dass Ollama lÃ¤uft: `ollama serve`

â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚- PrÃ¼fen Sie, ob Port 11434 verfÃ¼gbar ist

â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜- Starten Sie Ollama neu

        â”‚                                     â”‚

        â–¼                                     â–¼### Modell-Download schlÃ¤gt fehl

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”- PrÃ¼fen Sie Ihre Internetverbindung

â”‚  OllamaManager   â”‚              â”‚   UI-Widgets     â”‚- Stellen Sie sicher, dass genÃ¼gend Speicherplatz vorhanden ist

â”‚                  â”‚              â”‚                  â”‚- Versuchen Sie es mit einem kleineren Modell

â”‚ â€¢ API-Calls      â”‚              â”‚ â€¢ ColorWheel     â”‚

â”‚ â€¢ Streaming      â”‚              â”‚ â€¢ ChatBubble     â”‚### Anwendung startet nicht

â”‚ â€¢ Model-Mgmt     â”‚              â”‚ â€¢ Combobox       â”‚- PrÃ¼fen Sie, ob alle AbhÃ¤ngigkeiten installiert sind

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- Stellen Sie sicher, dass Sie Python 3.8+ verwenden

        â”‚

        â–¼## Entwicklung

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚        Ollama REST API                   â”‚Das Projekt ist in mehrere Klassen unterteilt:

â”‚      (http://localhost:11434)            â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- `OllamaManager`: Verwaltet die Ollama-API-Kommunikation

```- `LLMMessenger`: Hauptanwendung mit UI

- Threading fÃ¼r Non-Blocking-Operationen

### Datenfluss-Diagramm

---

```

User Input### ğŸ§¹ **Anti-Redundanz-System**

   â”‚Saubere, lesbare Ausgaben ohne nervige Wiederholungen:

   â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”#### **Download-Logging:**

â”‚  UI Event Handler                   â”‚- **Status-Filter:** Identische Status werden nicht wiederholt

â”‚  (send_message)                     â”‚- **Timing-Optimiert:** Progress-Updates nur alle 2 Sekunden  

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- **Kompakt:** Ein-Zeilen-Format statt Multi-Line-Spam

             â”‚- **Layer-Smart:** Neue Layer nur bei tatsÃ¤chlichem Wechsel

             â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”#### **Chat-Streaming:**

â”‚  Validation & Preprocessing         â”‚- **Rate-Limiting:** UI-Updates nur alle 0.1 Sekunden

â”‚  â€¢ Model-Check                      â”‚- **Duplikat-Erkennung:** Verhindert doppelte Nachrichten

â”‚  â€¢ Message-History                  â”‚- **Intelligente Ersetung:** Ersetzt nur Nachrichten vom gleichen Sender

â”‚  â€¢ BIAS-Integration                 â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜#### **Beispiel - Saubere Ausgabe:**

             â”‚```

             â–¼ğŸš€ DOWNLOAD START: llama2:13b

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”ğŸ“¡ Verwende Ollama Client fÃ¼r llama2:13b

â”‚  OllamaManager.chat()               â”‚â³ Starte Download-Stream...

â”‚  â€¢ API-Request                      â”‚ğŸ“¥ Status: pulling manifest

â”‚  â€¢ Stream-Processing                â”‚ğŸ”„ Layer: 2609048d349e

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ğŸ“Š 2.0% (140.7MB/7025.5MB) | 5.9MB/s | ETA: 19.3min

             â”‚ğŸ“Š 6.8% (477.1MB/7025.5MB) | 6.5MB/s | ETA: 16.8min

             â–¼ (Generator)âœ… DOWNLOAD COMPLETE: llama2:13b

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”```

â”‚  Progressive Update                 â”‚

â”‚  â€¢ Token-by-Token Display           â”‚**Resultat:** 90% weniger redundante Ausgaben! ğŸ¯

â”‚  â€¢ UI-Update (after_idle)           â”‚

â”‚  â€¢ Performance-Tracking             â”‚## ğŸ†• Erweiterte Features - Live-API und intelligente Kategorisierung

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

             â”‚### ğŸŒ **Live-Ollama-API Integration**

             â–¼Die Anwendung fragt jetzt **live die aktuellen Modelle** direkt von Ollama ab:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚  Session-Persistence                â”‚- **Echte Live-Daten:** Keine statische Liste mehr - immer die neuesten Modelle

â”‚  â€¢ Auto-Save Timer (60s)            â”‚- **Automatische Updates:** Neue Modelle erscheinen sofort nach Release  

â”‚  â€¢ JSON-Export                      â”‚- **Fallback-System:** Robuste Fallback-Liste bei API-Problemen

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜- **60+ Aktuelle Modelle:** Immer die vollstÃ¤ndige, aktuelle Ollama-Bibliothek

```

### ğŸ¨ **Intelligente GrÃ¶ÃŸen-Kategorisierung**

---Modelle sind jetzt **farblich gruppiert** nach RAM-Anforderungen:



## Installation#### ğŸŸ¢ **Klein (< 4GB RAM)** - 18 Modelle

Perfekt fÃ¼r schwÃ¤chere Hardware:

### Voraussetzungen- `tinyllama:1.1b`, `phi3:mini`, `gemma:2b`

- `orca-mini:3b`, `phi:2.7b`, `qwen2:0.5b`

| Komponente | Version | Beschreibung |

|------------|---------|--------------|#### ğŸŸ¡ **Mittel (4-8GB RAM)** - 32 Modelle  

| **Python** | 3.8+ | Programmiersprache |Standard-Modelle fÃ¼r normale Hardware:

| **Ollama** | Latest | Lokaler AI-Server |- `llama3.2:3b`, `mistral:7b`, `codellama:7b`

| **RAM** | 8GB+ | Mindestens fÃ¼r mittlere Modelle |- `gemma:7b`, `deepseek-coder:6.7b`, `phi3`

| **OS** | Windows/Linux/Mac | PlattformÃ¼bergreifend |

#### ğŸŸ  **GroÃŸ (8-16GB RAM)** - 3 Modelle

### Schritt 1: Ollama installierenFÃ¼r leistungsstarke Systeme:

- `llama2:13b`, `solar:10.7b`, `starcode:15b`

```bash

# Windows/Mac: Download von https://ollama.ai#### ğŸ”´ **Sehr GroÃŸ (16GB+ RAM)** - 7 Modelle

# Linux:FÃ¼r High-End-Hardware:

curl -fsSL https://ollama.ai/install.sh | sh- `llama2:70b`, `mixtral:8x7b`, `codellama:34b`

- `mixtral:8x22b`, `falcon:40b`

# Ollama starten

ollama serve### ğŸ›ï¸ **Verbessertes Interface**

```

#### **Kategorisiertes Dropdown-MenÃ¼:**

### Schritt 2: Repository klonen- **Farbkodierte Kategorien** mit Emoji-Indikation

- **Ãœbersichtliche Gruppierung** nach Hardware-Anforderungen

```bash- **Intelligente Auswahl** - Kategorie-Header sind nicht herunterladbar

git clone https://github.com/Nr44suessauer/Ki-whisperer.git- **Live-Feedback** - Zeigt Anzahl gefundener Modelle an

cd Ki-whisperer/a1_terminal_modular

```#### **Smarte Features:**

- **DuplikatsprÃ¼fung:** Warnt vor bereits installierten Modellen

### Schritt 3: Dependencies installieren- **Hardware-Hinweise:** Direkte RAM-Anforderungen sichtbar

- **Bessere Fehlermeldungen:** ErklÃ¤rt warum Auswahl ungÃ¼ltig ist

```bash- **Live-Updates:** "ğŸ”„ Lade aktuelle Modell-Liste von Ollama..."

pip install -r requirements.txt

```### ğŸš€ **Verwendung der neuen Live-Features**



**requirements.txt:**1. **Hardware-gerechte Auswahl:**

```   ```

customtkinter>=5.2.0   ğŸŸ¢ Schwache Hardware (4GB RAM)    â†’ WÃ¤hlen Sie aus "Klein"

ollama>=0.1.0   ğŸŸ¡ Normale Hardware (8GB RAM)     â†’ WÃ¤hlen Sie aus "Mittel"  

PyYAML>=6.0   ğŸŸ  Starke Hardware (16GB RAM)     â†’ WÃ¤hlen Sie aus "GroÃŸ"

requests>=2.31.0   ğŸ”´ High-End Hardware (32GB+ RAM)  â†’ WÃ¤hlen Sie aus "Sehr GroÃŸ"

```   ```



### Schritt 4: Erstes Modell herunterladen2. **Live-Updates nutzen:**

   - Klicken Sie "Aktualisieren" fÃ¼r neueste Modelle

```bash   - Die App holt automatisch die aktuelle Ollama-Bibliothek

# Beispiel: Llama 3.2 (empfohlen fÃ¼r Start)   - Neue Releases erscheinen sofort in der Liste

ollama pull llama3.2:3b

```3. **Kategorien durchsuchen:**

   - Ã–ffnen Sie das "VerfÃ¼gbare Modelle" Dropdown

---   - Scrollen Sie durch die farbkodierten Kategorien  

   - WÃ¤hlen Sie ein Modell (nicht die Kategorie-Header)

## Schnellstart   - Klicken Sie "AusgewÃ¤hltes Modell herunterladen"



### Windows (empfohlen)### ğŸ”§ **Technische Details**



```bash#### **Live-API Endpunkt:**

cd a1_terminal_modular```

start.batURL: https://registry.ollama.ai/v2/_catalog

```Methode: GET mit User-Agent Header

Timeout: 10 Sekunden mit Fallback

### Direkt mit Python```



```bash#### **Kategorisierungs-Logik:**

python main.py- **Automatische Erkennung** anhand Modellnamen (1.1b, 7b, 70b, etc.)

```- **Intelligente Gruppierung** nach RAM-Anforderungen

- **Fallback-Kategorisierung** fÃ¼r unbekannte GrÃ¶ÃŸen

### Erste Schritte

#### **Performance:**

1. **Modell auswÃ¤hlen** - Dropdown-MenÃ¼ im Chat-Tab- **Threading:** Alle API-Calls laufen im Hintergrund

2. **Nachricht senden** - Textfeld unten, Enter-Taste- **Caching:** Modell-Liste wird zwischengespeichert  

3. **BIAS setzen** (optional) - Tab "BIAS" fÃ¼r Session-Kontext- **Non-Blocking UI:** Interface bleibt wÃ¤hrend des Ladens reaktiv

4. **Session speichern** - Automatisch alle 60 Sekunden

Die erweiterte Version bietet jetzt **echte Live-Integration** mit der Ollama-Registry und macht es viel einfacher, das richtige Modell fÃ¼r Ihre Hardware zu finden! ğŸ¯

---

## Lizenz

## Modulare Struktur

MIT License - Siehe LICENSE Datei fÃ¼r Details.
### Core-Module

#### 1. `a1_terminal.py` (Hauptanwendung)

**Verantwortlichkeiten:**
- UI-Orchestrierung und Layout
- Event-Handling (Buttons, Eingaben)
- Session-Management (Laden, Speichern, Wechseln)
- Konfigurations-Management (YAML)
- Chat-Logik (Nachrichten senden/empfangen)
- Export-Funktionen (Markdown, JSON)

**Wichtige Methoden:**

| Methode | Beschreibung |
|---------|--------------|
| `__init__()` | Initialisierung, Config-Laden |
| `setup_ui()` | GUI-Erstellung (Tabs, Widgets) |
| `send_message()` | Nachricht an AI senden |
| `create_new_session()` | Neue Chat-Session erstellen |
| `save_session()` | Session persistieren |
| `apply_config()` | Konfiguration anwenden |

**Codebeispiel - Nachricht senden:**
```python
def send_message(self, event=None):
    message = self.message_entry.get()
    if not message.strip():
        return
    
    # Nachricht zur Historie hinzufÃ¼gen
    self.message_history.append(message)
    self.history_index = -1
    
    # User-Bubble anzeigen
    self.add_chat_bubble("Sie", message)
    
    # AI-Antwort in Thread generieren
    threading.Thread(
        target=self._generate_response,
        args=(message,),
        daemon=True
    ).start()
```

#### 2. `ollama_manager.py` (API-Client)

**Verantwortlichkeiten:**
- Kommunikation mit Ollama REST API
- Model-Management (Liste, Download, LÃ¶schen)
- Chat-FunktionalitÃ¤t (Streaming)
- Model-Kategorisierung nach GrÃ¶ÃŸe
- Verbindungs-Status-PrÃ¼fung

**Wichtige Methoden:**

| Methode | Beschreibung |
|---------|--------------|
| `is_ollama_running()` | PrÃ¼ft Ollama-Service-Status |
| `get_available_models()` | Listet installierte Modelle |
| `chat()` | Sendet Chat-Request (Generator) |
| `download_model()` | LÃ¤dt Modell herunter (Progress) |
| `categorize_models_by_size()` | Gruppiert Modelle nach RAM-Bedarf |

**Codebeispiel - Chat mit Streaming:**
```python
def chat(self, model, messages, stop_flag=None):
    """Generator fÃ¼r Streaming-Chat-Antworten"""
    try:
        response = self.client.chat(
            model=model,
            messages=messages,
            stream=True
        )
        
        for chunk in response:
            if stop_flag and stop_flag():
                break
            
            if 'message' in chunk:
                content = chunk['message'].get('content', '')
                if content:
                    yield content
    except Exception as e:
        yield f"Fehler: {str(e)}"
```

**Model-Kategorisierung:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸŸ¢ Klein (< 4GB RAM)                       â”‚
â”‚  â€¢ tinyllama:1.1b, phi3:mini, gemma:2b    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸŸ¡ Mittel (4-8GB RAM)                      â”‚
â”‚  â€¢ llama3.2:3b, mistral:7b, codellama:7b  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸŸ  GroÃŸ (8-16GB RAM)                       â”‚
â”‚  â€¢ llama2:13b, vicuna:13b, solar:10.7b    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ”´ Sehr GroÃŸ (16GB+ RAM)                   â”‚
â”‚  â€¢ llama2:70b, mixtral:8x7b, codellama:34bâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### UI-Module

#### 3. `color_wheel.py` (FarbwÃ¤hler)

**Features:**
- Interaktiver HSV-Farbkreis
- RGB-Hex-Ausgabe
- Marker fÃ¼r ausgewÃ¤hlte Farbe
- Callback-System fÃ¼r FarbÃ¤nderungen

**Technische Details:**
```python
# HSV zu RGB Konvertierung
def hsv_to_rgb(self, h, s, v):
    h = h / 360.0
    c = v * s
    x = c * (1 - abs((h * 6) % 2 - 1))
    m = v - c
    
    # Sextant-basierte Konvertierung
    # ... (6 Sektoren fÃ¼r Farbkreis)
    
    return (int((r + m) * 255), 
            int((g + m) * 255), 
            int((b + m) * 255))
```

#### 4. `chat_bubble.py` (Nachricht)

**Features:**
- Dynamische HÃ¶henberechnung
- Kopier-FunktionalitÃ¤t
- Sender-spezifisches Styling
- Konfigurierbares Layout

**Styling-Logik:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Sender: Sie                              â”‚
â”‚ â€¢ Rechts ausgerichtet                    â”‚
â”‚ â€¢ Matrix-Farben (DunkelgrÃ¼n + NeongrÃ¼n) â”‚
â”‚ â€¢ Courier New Font                       â”‚
â”‚ â€¢ Border-Effekt                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sender: AI (ğŸ¤–)                          â”‚
â”‚ â€¢ Links ausgerichtet                     â”‚
â”‚ â€¢ Blau-TÃ¶ne                              â”‚
â”‚ â€¢ Consolas Font (Code)                   â”‚
â”‚ â€¢ Kein Border                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sender: System                           â”‚
â”‚ â€¢ Links ausgerichtet                     â”‚
â”‚ â€¢ Rot-TÃ¶ne (Warnung)                     â”‚
â”‚ â€¢ Arial Font                             â”‚
â”‚ â€¢ Kompakte Darstellung                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 5. `categorized_combobox.py` (Dropdown)

**Features:**
- Hierarchische Darstellung
- Kategorie-Header (nicht auswÃ¤hlbar)
- Flattening fÃ¼r KompatibilitÃ¤t

**Struktur:**
```python
categories = {
    "ğŸŸ¢ Klein (< 4GB RAM)": ["phi:2.7b", "gemma:2b"],
    "ğŸŸ¡ Mittel (4-8GB RAM)": ["llama3.2:3b", "mistral:7b"],
    # ...
}

# Flat-Liste fÃ¼r CTkComboBox:
# ["--- ğŸŸ¢ Klein (< 4GB RAM) ---", "phi:2.7b", "gemma:2b",
#  "--- ğŸŸ¡ Mittel (4-8GB RAM) ---", "llama3.2:3b", ...]
```

---

## Technische Dokumentation

### Session-Management

#### Session-Format (JSON)

```json
{
  "session_id": "session_20251109_203448_442",
  "created_at": "2025-11-09T20:34:48",
  "modified_at": "2025-11-09T21:15:32",
  "model": "llama3.2:3b",
  "bias": "Du bist ein hilfreicher Assistent fÃ¼r Python-Programmierung.",
  "messages": [
    {
      "role": "user",
      "content": "Wie funktioniert Multithreading?",
      "timestamp": "2025-11-09T20:35:12"
    },
    {
      "role": "assistant",
      "content": "Multithreading ermÃ¶glicht...",
      "timestamp": "2025-11-09T20:35:18"
    }
  ],
  "statistics": {
    "message_count": 12,
    "total_tokens": 3420,
    "average_response_time": 2.3
  }
}
```

#### Session-Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CREATE SESSION                               â”‚
â”‚    â€¢ Generate unique ID (timestamp + random)    â”‚
â”‚    â€¢ Initialize empty message list              â”‚
â”‚    â€¢ Set default BIAS                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ACTIVE SESSION                               â”‚
â”‚    â€¢ Add messages to history                    â”‚
â”‚    â€¢ Update statistics                          â”‚
â”‚    â€¢ Auto-save every 60s                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SAVE SESSION                                 â”‚
â”‚    â€¢ Serialize to JSON                          â”‚
â”‚    â€¢ Write to sessions/ directory               â”‚
â”‚    â€¢ Update modified_at timestamp               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. LOAD SESSION                                 â”‚
â”‚    â€¢ Deserialize from JSON                      â”‚
â”‚    â€¢ Restore chat history                       â”‚
â”‚    â€¢ Recreate UI bubbles                        â”‚
â”‚    â€¢ Set model and BIAS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### BIAS-System

**BIAS** = Background Information And System instructions

Das BIAS-System ermÃ¶glicht Session-spezifischen Kontext:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BIAS Input (User)                              â”‚
â”‚ "Du bist ein Python-Experte..."               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Integration in jeden Request                   â”‚
â”‚                                                â”‚
â”‚ messages = [                                   â”‚
â”‚   {"role": "system", "content": BIAS},        â”‚
â”‚   {"role": "user", "content": "Frage 1"},    â”‚
â”‚   {"role": "assistant", "content": "..."},   â”‚
â”‚   {"role": "user", "content": "Frage 2"}     â”‚
â”‚ ]                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**BIAS Best Practices:**

| Anwendungsfall | Beispiel |
|----------------|----------|
| **Rollenspiel** | "Du bist ein Senior-Developer mit 10 Jahren Erfahrung." |
| **Spezialwissen** | "Du bist Experte fÃ¼r Cybersecurity und Penetration Testing." |
| **TonalitÃ¤t** | "Antworte immer knapp und prÃ¤zise, ohne ErklÃ¤rungen." |
| **Format** | "Gib Code-Beispiele immer in Markdown-Format aus." |

### Konfigurations-System

#### YAML-Struktur

```yaml
# Farben fÃ¼r Chat-Bubbles (Hex-Codes)
user_bg_color: "#003300"      # DunkelgrÃ¼n
user_text_color: "#00FF00"    # NeongrÃ¼n
ai_bg_color: "#1E3A5F"        # Dunkelblau
ai_text_color: "white"
system_bg_color: "#722F37"    # Dunkelrot
system_text_color: "white"

# Schriftarten
user_font: "Courier New"      # Matrix-Style
user_font_size: 11
ai_font: "Consolas"           # Code-Font
ai_font_size: 11
system_font: "Arial"
system_font_size: 10

# UI-Optionen
show_system_messages: true    # System-Nachrichten anzeigen
```

#### Live-Update-Mechanismus

```python
def apply_config(self):
    """Wendet Konfiguration ohne Neustart an"""
    
    # 1. Config in YAML speichern
    self.save_config()
    
    # 2. Alle Chat-Bubbles aktualisieren
    for bubble in self.chat_bubbles:
        bubble.update_style(self.config)
    
    # 3. UI-Elemente refreshen
    self.chat_display.update()
    
    # 4. Vorschau-Icons aktualisieren
    self.update_color_previews()
```

---

## Features

### 1. Model-Management

#### Modell-Download

```python
# Download mit Progress-Tracking
def download_model(self, model_name, progress_callback=None):
    """
    Args:
        model_name: z.B. "llama3.2:3b"
        progress_callback: Funktion(current, total, status)
    """
    try:
        for progress in ollama.pull(model_name, stream=True):
            if progress_callback:
                progress_callback(
                    progress.get('completed', 0),
                    progress.get('total', 0),
                    progress.get('status', '')
                )
    except Exception as e:
        raise Exception(f"Download fehlgeschlagen: {e}")
```

**UI-Darstellung:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Modell herunterladen                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model: llama3.2:3b                       â”‚
â”‚                                          â”‚
â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 60%                 â”‚
â”‚ 1.2 GB / 2.0 GB                          â”‚
â”‚                                          â”‚
â”‚ Status: Pulling manifest                 â”‚
â”‚                                          â”‚
â”‚ [ Abbrechen ]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. Chat-FunktionalitÃ¤t

#### Echtzeit-Streaming

```
User Message
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thread-Start                â”‚
â”‚ _generate_response()        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API-Call (Generator)        â”‚
â”‚ ollama_manager.chat()       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ (Yields Tokens)
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Progressive Update          â”‚
â”‚ â€¢ Append to buffer          â”‚
â”‚ â€¢ UI update (after_idle)    â”‚
â”‚ â€¢ 100ms throttle            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Complete Response           â”‚
â”‚ â€¢ Final bubble update       â”‚
â”‚ â€¢ Add to history            â”‚
â”‚ â€¢ Trigger auto-save         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Nachricht-Historie (Pfeiltasten-Navigation)

```python
# â†‘ Pfeiltaste = Vorherige Nachricht
# â†“ Pfeiltaste = NÃ¤chste Nachricht

def on_up_arrow(self, event):
    if self.message_history:
        self.history_index += 1
        if self.history_index < len(self.message_history):
            msg = self.message_history[-(self.history_index + 1)]
            self.message_entry.delete(0, tk.END)
            self.message_entry.insert(0, msg)
```

### 3. Export-Funktionen

#### Markdown-Export

```markdown
# Session: session_20251109_203448_442
**Erstellt:** 2025-11-09 20:34:48
**Modell:** llama3.2:3b

## BIAS
Du bist ein hilfreicher Python-Assistent.

---

### Sie (20:35:12)
Wie funktioniert Multithreading in Python?

### ğŸ¤– llama3.2:3b (20:35:18)
Multithreading in Python ermÃ¶glicht...
```

#### JSON-Export

VollstÃ¤ndiger Export aller Session-Daten (siehe [Session-Format](#session-format-json)).

### 4. Anpassbare UI

#### Farbschemas

**Matrix-Theme (Standard):**
```yaml
user_bg_color: "#003300"
user_text_color: "#00FF00"
ai_bg_color: "#1E3A5F"
ai_text_color: "white"
```

**Midnight-Theme:**
```yaml
user_bg_color: "#1a1a2e"
user_text_color: "#eee"
ai_bg_color: "#16213e"
ai_text_color: "#0f3460"
```

**Solarized-Theme:**
```yaml
user_bg_color: "#002b36"
user_text_color: "#839496"
ai_bg_color: "#073642"
ai_text_color: "#93a1a1"
```

---

## Konfiguration

### Config-Tab (UI)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ âš™ï¸ Konfiguration                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚ â”Œâ”€ Sie (User) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Hintergrund: [#003300] ğŸ¨                     â”‚ â”‚
â”‚ â”‚ Text:        [#00FF00] ğŸ¨                     â”‚ â”‚
â”‚ â”‚ Schriftart:  [Courier New â–¼] GrÃ¶ÃŸe: [11]     â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                    â”‚
â”‚ â”Œâ”€ AI-Modell â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Hintergrund: [#1E3A5F] ğŸ¨                     â”‚ â”‚
â”‚ â”‚ Text:        [white   ] ğŸ¨                     â”‚ â”‚
â”‚ â”‚ Schriftart:  [Consolas â–¼] GrÃ¶ÃŸe: [11]        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                    â”‚
â”‚ â”Œâ”€ System-Nachrichten â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Hintergrund: [#722F37] ğŸ¨                     â”‚ â”‚
â”‚ â”‚ Text:        [white   ] ğŸ¨                     â”‚ â”‚
â”‚ â”‚ Schriftart:  [Arial â–¼] GrÃ¶ÃŸe: [10]           â”‚ â”‚
â”‚ â”‚ Anzeigen:    [âœ“]                              â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                    â”‚
â”‚         [ Anwenden ]  [ Standard ]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## API-Referenz

### OllamaManager

#### Konstruktor
```python
manager = OllamaManager()
```

#### Methoden

##### `is_ollama_running() -> bool`
PrÃ¼ft ob Ollama-Service lÃ¤uft.

**Returns:** `True` wenn erreichbar, sonst `False`

---

##### `get_available_models() -> List[str]`
Holt Liste aller installierten Modelle.

**Returns:** Liste von Modellnamen

---

##### `chat(model: str, messages: List[dict], stop_flag: Callable = None) -> Generator`
Sendet Chat-Request mit Streaming.

**Parameters:**
- `model`: Modellname (z.B. "llama3.2:3b")
- `messages`: Liste von Message-Dicts
- `stop_flag`: Optional - Funktion die `True` zurÃ¼ckgibt zum Stoppen

**Yields:** Token-Strings (einzelne Text-Chunks)

---

## Performance & Monitoring

### Performance-Metriken

| Metrik | Beschreibung | Typischer Wert |
|--------|--------------|----------------|
| **Response Time** | Zeit bis erste Token | 0.5-2.0s |
| **Token Rate** | Tokens pro Sekunde | 20-50 tokens/s |
| **Memory Usage** | RAM-Verbrauch App | 100-200 MB |
| **Model Memory** | RAM-Verbrauch Modell | 2-16 GB |

---

## Troubleshooting

### Problem: "Ollama nicht erreichbar"

**LÃ¶sung:**
```bash
# Ollama starten
ollama serve

# Status prÃ¼fen
curl http://localhost:11434/api/tags
```

---

### Problem: UI friert ein

**LÃ¶sung:**
```yaml
# In config erhÃ¶hen:
performance:
  update_throttle_ms: 200  # Standard: 100
```

---

## Beste Praktiken

### Model-Auswahl

| Anwendungsfall | Empfohlenes Modell | BegrÃ¼ndung |
|----------------|-------------------|------------|
| **Schnelle Tests** | `phi3:mini` (1.5GB) | Extrem schnell |
| **Allgemein** | `llama3.2:3b` (2GB) | Guter Kompromiss |
| **Code** | `codellama:7b` (4GB) | Optimiert fÃ¼r Programmierung |
| **Lange Texte** | `mistral:7b` (4GB) | GroÃŸes Kontext-Fenster |
| **Maximale QualitÃ¤t** | `llama2:70b` (40GB) | Beste QualitÃ¤t |

---

## Lizenz

MIT License

---

## Credits

**Entwickelt von:** Nr44suessauer  
**Framework:** CustomTkinter  
**AI-Backend:** Ollama  

---

**Version:** 2.0.0  
**Last Updated:** 2025-11-09

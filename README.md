# A1-Terminal

**Professional Chat Client for Local AI Models via Ollama**

Version 2.0 - Modular Architecture

---

## ğŸš€ Fully Automatic Installation

### One Command - Everything Installed!

**Windows:**
```powershell
# Run as Administrator (Right-click -> "Run as Administrator")
.\scripts\install.bat
```

**Linux/macOS:**
```bash
chmod +x scripts/install.sh
./scripts/install.sh
```

### What Gets Installed?

The script installs **fully automatically**:
- âœ… Python 3.8+ (if not present)
- âœ… All Python packages (CustomTkinter, ollama, PyYAML, requests, pyperclip)
- âœ… **Ollama** (completely automatic, **no manual installation needed!**)
- âœ… Test model **tinyllama:1.1b** (~600 MB, ready to use immediately)

### Start After Installation

```powershell
.\start.bat          # Windows (from main folder)
```

**Done!** The app starts with a working test model. ğŸ‰

---

## âœ¨ Features

- ğŸ¯ **Modular Architecture** - Clean code structure
- ğŸš€ **Real-time Streaming** - Live display of AI responses
- ğŸ’¾ **Session Management** - Save & load chats
- ğŸ¨ **Fully Customizable** - Colors, fonts, layout
- ğŸ“Š **Model Management** - Download & categorization of models
- ğŸ”„ **100% Offline** - All models run locally, no cloud
- âš¡ **Stop Function** - Generation can be interrupted at any time
- ğŸ“ **BIAS System** - System prompts for AI control

---

## ğŸ“ Project Structure

The project is now **modularly** structured:

```
A1-Terminal/
â”œâ”€â”€ start.bat                   # Windows: Quick start
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ DOCUMENTATION_EN.md         # Complete technical documentation
â”‚
â”œâ”€â”€ scripts/                    # Installation scripts
â”‚   â”œâ”€â”€ install.bat             # Windows installation
â”‚   â””â”€â”€ install.sh              # Linux/macOS installation
â”‚
â””â”€â”€ a1_terminal_modular/        # Main application
    â”œâ”€â”€ main.py                 # Entry point
    â”œâ”€â”€ requirements.txt        # Python dependencies
    â”œâ”€â”€ a1_terminal_config.yaml # Configuration
    â”‚
    â”œâ”€â”€ sessions/               # Saved chat sessions
    â”‚
    â””â”€â”€ src/                    # Source code
        â”œâ”€â”€ core/               # Core logic
        â”‚   â”œâ”€â”€ a1_terminal.py      # Main class
        â”‚   â””â”€â”€ ollama_manager.py   # Ollama API client
        â”‚
        â””â”€â”€ ui/                 # UI components
            â”œâ”€â”€ chat_bubble.py
            â”œâ”€â”€ session_card.py
            â”œâ”€â”€ model_selector.py
            â””â”€â”€ ...
```

---

## ğŸ“– Documentation

**[Complete Technical Documentation (DOCUMENTATION_EN.md)](./DOCUMENTATION_EN.md)**

Contains:
- âš™ï¸ Detailed architecture description
- ğŸ“¡ API reference & Ollama integration
- ğŸ¨ Configuration options
- ğŸ‘¨â€ğŸ’» Developer guide
- ğŸ› Troubleshooting & problem solving

---

## ğŸ’¡ Recommended Models

After installation, **tinyllama:1.1b** is already installed. You can download more models in the "Models" tab of the app:

| Model | Size | RAM | Description |
|--------|-------|-----|--------------|
| **tinyllama:1.1b** | 600 MB | 4 GB | âœ… Already installed! Very fast |
| phi3:mini | 2 GB | 8 GB | Good quality, balanced |
| llama3.2:3b | 2 GB | 8 GB | Latest version, very good |
| mistral:7b | 4 GB | 12 GB | High quality |
| codellama:7b | 4 GB | 12 GB | Specialized for code |

---

## ğŸ® GPU/CUDA Support (Optional)

**A1-Terminal automatically uses your NVIDIA GPU if available - no code changes needed!**

### Benefits with CUDA:
- âš¡ **Much faster inference** - Models respond 5-10x faster
- ğŸš€ **Larger models** - Run 13B+ models smoothly
- ğŸ’¾ **Less RAM usage** - GPU VRAM is used instead of system RAM

### Setup:

1. **Install NVIDIA GPU drivers** from [nvidia.com/drivers](https://www.nvidia.com/drivers)
2. **That's it!** Ollama automatically detects and uses your GPU

### Verify GPU usage:
```powershell
# Check if GPU is being used (run while model is active)
nvidia-smi
```

**No configuration needed in A1-Terminal** - Ollama handles everything automatically! ğŸ‰

---

## ğŸ”§ System Requirements

**Minimum:**
- Windows 10/11, Linux (Ubuntu 20.04+), macOS 11+
- 8 GB RAM
- 10 GB free storage
- Internet connection (only for installation)

**Recommended:**
- 16 GB RAM (for larger models)
- 50 GB free storage (for multiple models)

---

## ğŸš€ Quick Start

### Automatic Installation (Recommended)

**Windows:**
```powershell
# Run as Administrator (Right-click -> "Run as Administrator")
.\scripts\install.bat
```

**Linux/macOS:**
```bash
chmod +x scripts/install.sh
./scripts/install.sh
```

The installation script automatically installs:
- âœ… Python 3.8+ (if not present)
- âœ… All Python packages (CustomTkinter, ollama, etc.)
- âœ… Ollama
- âœ… Optional: Test model (tinyllama, phi3, llama3.2)

### Manual Installation

<details>
<summary>Click to show manual installation</summary>

#### 1. Install Ollama

Visit [ollama.ai](https://ollama.ai) and install Ollama.

#### 2. Install Dependencies

```powershell
pip install -r a1_terminal_modular\requirements.txt
```

#### 3. Start

```powershell
.\start.bat
```

</details>

### After Installation

**Windows:**
```powershell
.\start.bat
```

**Linux/macOS:**
```bash
./start.bat
```

---

## ğŸ¤ Support

For problems see:
- ğŸ“– [Troubleshooting in the documentation](./DOCUMENTATION_EN.md#troubleshooting)
- ğŸ› [GitHub Issues](https://github.com/Nr44suessauer/A1-Terminal/issues)

---

**Have fun with A1-Terminal! ğŸš€**

*Completely automatic installation â€¢ No manual configuration â€¢ Ready to use immediately*

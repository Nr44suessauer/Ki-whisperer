# LLM Messenger - Ollama Chat Client

Ein moderner Python-Chat-Client fÃ¼r lokale AI-Modelle mit Ollama-Integration.

## Features

- ğŸ¨ Modernes, dunkles UI mit CustomTkinter
- ğŸ“¥ Modell-Download direkt aus der Anwendung
- ğŸ—‘ï¸ Modell-Verwaltung (LÃ¶schen, AuswÃ¤hlen)
- ğŸ’¬ Streaming-Chat mit AI-Modellen
- ğŸ“ Chat-Historie innerhalb einer Session
- ğŸ”„ Live-Status von Ollama
- âš¡ Echtzeit-Updates

## Voraussetzungen

1. **Ollama installieren**
   - Gehen Sie zu [ollama.ai](https://ollama.ai) 
   - Laden Sie Ollama fÃ¼r Ihr Betriebssystem herunter
   - Installieren und starten Sie Ollama

2. **Python 3.8+ installiert**

## Installation

1. **Repository klonen oder herunterladen**
   ```bash
   cd "C:\Users\marcn\Documents\LLM Messenger"
   ```

2. **AbhÃ¤ngigkeiten installieren**
   ```bash
   C:/Users/marcn/AppData/Local/Programs/Python/Python312/python.exe -m pip install -r requirements.txt
   ```

## Verwendung

1. **Ollama starten** (falls noch nicht gestartet)
   ```bash
   ollama serve
   ```

2. **LLM Messenger starten**
   ```bash
   C:/Users/marcn/AppData/Local/Programs/Python/Python312/python.exe llm_messenger.py
   ```

## Erste Schritte

1. **Modell herunterladen**
   - Klicken Sie auf "Modell herunterladen"
   - Geben Sie einen Modellnamen ein (z.B. `llama2`, `mistral`, `codellama`)
   - Warten Sie, bis der Download abgeschlossen ist

2. **Modell auswÃ¤hlen**
   - WÃ¤hlen Sie das gewÃ¼nschte Modell aus dem Dropdown-MenÃ¼

3. **Chatten**
   - Geben Sie Ihre Nachricht ein und drÃ¼cken Sie Enter oder klicken Sie "Senden"
   - Die AI antwortet in Echtzeit

## Beliebte Modelle

- **llama2** - Allzweck-Sprachmodell von Meta
- **mistral** - Schnelles und effizientes Modell
- **codellama** - Spezialisiert auf Programmierung
- **phi** - Kleines, aber leistungsstarkes Modell
- **gemma** - Google's offenes Modell

## Funktionen

### Modell-Management
- **Download**: Laden Sie neue Modelle direkt herunter
- **LÃ¶schen**: Entfernen Sie nicht benÃ¶tigte Modelle
- **AuswÃ¤hlen**: Wechseln Sie zwischen verschiedenen Modellen

### Chat-Features
- **Streaming**: Sehen Sie die Antwort in Echtzeit
- **Historie**: Chat-Verlauf bleibt wÃ¤hrend der Session erhalten
- **Zeitstempel**: Alle Nachrichten haben Zeitstempel
- **System-Meldungen**: Informationen Ã¼ber Status und Fehler

## Fehlerbehebung

### Ollama nicht verbunden
- Stellen Sie sicher, dass Ollama lÃ¤uft: `ollama serve`
- PrÃ¼fen Sie, ob Port 11434 verfÃ¼gbar ist
- Starten Sie Ollama neu

### Modell-Download schlÃ¤gt fehl
- PrÃ¼fen Sie Ihre Internetverbindung
- Stellen Sie sicher, dass genÃ¼gend Speicherplatz vorhanden ist
- Versuchen Sie es mit einem kleineren Modell

### Anwendung startet nicht
- PrÃ¼fen Sie, ob alle AbhÃ¤ngigkeiten installiert sind
- Stellen Sie sicher, dass Sie Python 3.8+ verwenden

## Entwicklung

Das Projekt ist in mehrere Klassen unterteilt:

- `OllamaManager`: Verwaltet die Ollama-API-Kommunikation
- `LLMMessenger`: Hauptanwendung mit UI
- Threading fÃ¼r Non-Blocking-Operationen

---

### ğŸ§¹ **Anti-Redundanz-System**
Saubere, lesbare Ausgaben ohne nervige Wiederholungen:

#### **Download-Logging:**
- **Status-Filter:** Identische Status werden nicht wiederholt
- **Timing-Optimiert:** Progress-Updates nur alle 2 Sekunden  
- **Kompakt:** Ein-Zeilen-Format statt Multi-Line-Spam
- **Layer-Smart:** Neue Layer nur bei tatsÃ¤chlichem Wechsel

#### **Chat-Streaming:**
- **Rate-Limiting:** UI-Updates nur alle 0.1 Sekunden
- **Duplikat-Erkennung:** Verhindert doppelte Nachrichten
- **Intelligente Ersetung:** Ersetzt nur Nachrichten vom gleichen Sender

#### **Beispiel - Saubere Ausgabe:**
```
ğŸš€ DOWNLOAD START: llama2:13b
ğŸ“¡ Verwende Ollama Client fÃ¼r llama2:13b
â³ Starte Download-Stream...
ğŸ“¥ Status: pulling manifest
ğŸ”„ Layer: 2609048d349e
ğŸ“Š 2.0% (140.7MB/7025.5MB) | 5.9MB/s | ETA: 19.3min
ğŸ“Š 6.8% (477.1MB/7025.5MB) | 6.5MB/s | ETA: 16.8min
âœ… DOWNLOAD COMPLETE: llama2:13b
```

**Resultat:** 90% weniger redundante Ausgaben! ğŸ¯

## ğŸ†• Erweiterte Features - Live-API und intelligente Kategorisierung

### ğŸŒ **Live-Ollama-API Integration**
Die Anwendung fragt jetzt **live die aktuellen Modelle** direkt von Ollama ab:

- **Echte Live-Daten:** Keine statische Liste mehr - immer die neuesten Modelle
- **Automatische Updates:** Neue Modelle erscheinen sofort nach Release  
- **Fallback-System:** Robuste Fallback-Liste bei API-Problemen
- **60+ Aktuelle Modelle:** Immer die vollstÃ¤ndige, aktuelle Ollama-Bibliothek

### ğŸ¨ **Intelligente GrÃ¶ÃŸen-Kategorisierung**
Modelle sind jetzt **farblich gruppiert** nach RAM-Anforderungen:

#### ğŸŸ¢ **Klein (< 4GB RAM)** - 18 Modelle
Perfekt fÃ¼r schwÃ¤chere Hardware:
- `tinyllama:1.1b`, `phi3:mini`, `gemma:2b`
- `orca-mini:3b`, `phi:2.7b`, `qwen2:0.5b`

#### ğŸŸ¡ **Mittel (4-8GB RAM)** - 32 Modelle  
Standard-Modelle fÃ¼r normale Hardware:
- `llama3.2:3b`, `mistral:7b`, `codellama:7b`
- `gemma:7b`, `deepseek-coder:6.7b`, `phi3`

#### ğŸŸ  **GroÃŸ (8-16GB RAM)** - 3 Modelle
FÃ¼r leistungsstarke Systeme:
- `llama2:13b`, `solar:10.7b`, `starcode:15b`

#### ğŸ”´ **Sehr GroÃŸ (16GB+ RAM)** - 7 Modelle
FÃ¼r High-End-Hardware:
- `llama2:70b`, `mixtral:8x7b`, `codellama:34b`
- `mixtral:8x22b`, `falcon:40b`

### ğŸ›ï¸ **Verbessertes Interface**

#### **Kategorisiertes Dropdown-MenÃ¼:**
- **Farbkodierte Kategorien** mit Emoji-Indikation
- **Ãœbersichtliche Gruppierung** nach Hardware-Anforderungen
- **Intelligente Auswahl** - Kategorie-Header sind nicht herunterladbar
- **Live-Feedback** - Zeigt Anzahl gefundener Modelle an

#### **Smarte Features:**
- **DuplikatsprÃ¼fung:** Warnt vor bereits installierten Modellen
- **Hardware-Hinweise:** Direkte RAM-Anforderungen sichtbar
- **Bessere Fehlermeldungen:** ErklÃ¤rt warum Auswahl ungÃ¼ltig ist
- **Live-Updates:** "ğŸ”„ Lade aktuelle Modell-Liste von Ollama..."

### ğŸš€ **Verwendung der neuen Live-Features**

1. **Hardware-gerechte Auswahl:**
   ```
   ğŸŸ¢ Schwache Hardware (4GB RAM)    â†’ WÃ¤hlen Sie aus "Klein"
   ğŸŸ¡ Normale Hardware (8GB RAM)     â†’ WÃ¤hlen Sie aus "Mittel"  
   ğŸŸ  Starke Hardware (16GB RAM)     â†’ WÃ¤hlen Sie aus "GroÃŸ"
   ğŸ”´ High-End Hardware (32GB+ RAM)  â†’ WÃ¤hlen Sie aus "Sehr GroÃŸ"
   ```

2. **Live-Updates nutzen:**
   - Klicken Sie "Aktualisieren" fÃ¼r neueste Modelle
   - Die App holt automatisch die aktuelle Ollama-Bibliothek
   - Neue Releases erscheinen sofort in der Liste

3. **Kategorien durchsuchen:**
   - Ã–ffnen Sie das "VerfÃ¼gbare Modelle" Dropdown
   - Scrollen Sie durch die farbkodierten Kategorien  
   - WÃ¤hlen Sie ein Modell (nicht die Kategorie-Header)
   - Klicken Sie "AusgewÃ¤hltes Modell herunterladen"

### ğŸ”§ **Technische Details**

#### **Live-API Endpunkt:**
```
URL: https://registry.ollama.ai/v2/_catalog
Methode: GET mit User-Agent Header
Timeout: 10 Sekunden mit Fallback
```

#### **Kategorisierungs-Logik:**
- **Automatische Erkennung** anhand Modellnamen (1.1b, 7b, 70b, etc.)
- **Intelligente Gruppierung** nach RAM-Anforderungen
- **Fallback-Kategorisierung** fÃ¼r unbekannte GrÃ¶ÃŸen

#### **Performance:**
- **Threading:** Alle API-Calls laufen im Hintergrund
- **Caching:** Modell-Liste wird zwischengespeichert  
- **Non-Blocking UI:** Interface bleibt wÃ¤hrend des Ladens reaktiv

Die erweiterte Version bietet jetzt **echte Live-Integration** mit der Ollama-Registry und macht es viel einfacher, das richtige Modell fÃ¼r Ihre Hardware zu finden! ğŸ¯

## Lizenz

MIT License - Siehe LICENSE Datei fÃ¼r Details.